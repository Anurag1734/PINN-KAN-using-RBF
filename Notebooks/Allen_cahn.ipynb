{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Physics-Informed Neural Network with Kolmogorov-Arnold Networks (PINN-KAN) for Allen–Cahn Equation"
      ],
      "metadata": {
        "id": "GLopw_Dj_0vg"
      },
      "id": "GLopw_Dj_0vg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We use:\n",
        "- **PINNs (Physics-Informed Neural Networks)** to incorporate physical laws into the training process,\n",
        "- **KANs (Kolmogorov–Arnold Networks)** as flexible and interpretable neural function approximators,\n",
        "- **Radial Basis Functions (RBFs)** for effective spatial feature representation.\n",
        "\n",
        "---\n",
        "\n",
        "### Notebook Structure\n",
        "\n",
        "This notebook is organized into ten major stages:\n",
        "\n",
        "1. **RBF Components** – Define radial basis kernels for spatial encoding  \n",
        "2. **Model Architectures** – Build KAN and PINN models  \n",
        "3. **Physics Residuals** – Formulate the Allen–Cahn PDE residual  \n",
        "4. **Loss Functions** – Combine PDE, boundary, and initial condition losses  \n",
        "5. **Training** – Optimize the model using gradient-based methods  \n",
        "6. **Evaluation** – Assess model accuracy and residual consistency  \n",
        "7. **Visualization** – Compare predicted vs. true field solutions  \n",
        "8. **Main Pipeline** – Integrate all steps into a unified workflow  \n",
        "9. **Run Experiment** – Execute multiple configurations for comparison  \n",
        "10. **Diagnostics & Summary** – Analyze, compare, and interpret final results  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oNDAgjhY_6CW"
      },
      "id": "oNDAgjhY_6CW"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import os\n",
        "import pickle\n",
        "from scipy.interpolate import griddata\n",
        "import logging\n",
        "\n",
        "# Configure a module-level logger and ensure we don't add multiple handlers\n",
        "logger = logging.getLogger(\"pinn_logger\")\n",
        "if not logger.handlers:\n",
        "    handler = logging.StreamHandler()\n",
        "    formatter = logging.Formatter(\"%(message)s\")\n",
        "    handler.setFormatter(formatter)\n",
        "    logger.addHandler(handler)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D23tmPdyAogM",
        "outputId": "756c9f0a-2b04-4a99-ac42-0a327c8b53da"
      },
      "id": "D23tmPdyAogM",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Radial Basis Function (RBF) Components\n",
        "We begin by defining the RBF kernels that help represent the spatial structure of the Allen–Cahn field.\n",
        "\n",
        "These functions will later be used to build flexible spatial embeddings for the model inputs.\n",
        "\n",
        "**Key Concepts:**\n",
        "- RBFs are used to map spatial inputs into higher-dimensional feature spaces.\n",
        "- Useful for capturing local patterns and smooth variations.\n"
      ],
      "metadata": {
        "id": "93QhqCnc__Qn"
      },
      "id": "93QhqCnc__Qn"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RBFEdge(nn.Module):\n",
        "    \"\"\"Gaussian Radial Basis Function layer.\"\"\"\n",
        "    def __init__(self, input_dim: int, num_rbfs: int):\n",
        "        super().__init__()\n",
        "        self.centers = nn.Parameter(torch.randn(num_rbfs, input_dim))\n",
        "        self.sigmas = nn.Parameter(torch.ones(num_rbfs, input_dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        expanded = (x.unsqueeze(1) - self.centers) ** 2\n",
        "        scaled = expanded / (2 * (self.sigmas ** 2))\n",
        "        return torch.exp(-scaled.sum(dim=-1))\n",
        "\n",
        "\n",
        "class KANLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    KANLayer: maps from input_dim -> output_dim using an RBF expansion\n",
        "    num_rbfs controls the size of the intermediate RBF feature map.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, num_rbfs: int, output_dim: int):\n",
        "        super().__init__()\n",
        "        self.rbf_edge = RBFEdge(input_dim=input_dim, num_rbfs=num_rbfs)\n",
        "        self.linear = nn.Linear(num_rbfs, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        phi = self.rbf_edge(x)\n",
        "        return self.linear(phi)"
      ],
      "metadata": {
        "id": "5ubExpYjAZvu"
      },
      "id": "5ubExpYjAZvu",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Model Architectures\n",
        "In this section, we define the model architectures for both:\n",
        "- **Kolmogorov–Arnold Network (KAN)** layers  \n",
        "- **Physics-Informed Neural Network (PINN)** components  \n",
        "\n",
        "These models approximate the solution to the PDE while embedding physics constraints directly into the training objective.\n"
      ],
      "metadata": {
        "id": "tYoaeH8VACVG"
      },
      "id": "tYoaeH8VACVG"
    },
    {
      "cell_type": "code",
      "source": [
        "class PINN_KAN(nn.Module):\n",
        "    \"\"\"\n",
        "    Research-grade Physics-Informed Kolmogorov–Arnold Network:\n",
        "        - RBF + linear KAN layers\n",
        "        - LayerNorm for stable PDE training\n",
        "        - Tanh activation for smooth second derivatives\n",
        "        - Skip-connection (KAN-style functional bypass)\n",
        "        - More expressive RBF dimensions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim: int = 2,\n",
        "                 num_rbfs_list: List[int] = [32, 48, 32],\n",
        "                 out_dim: int = 1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        in_dim = input_dim\n",
        "\n",
        "        for num_rbfs in num_rbfs_list:\n",
        "            layers.append(KANLayer(\n",
        "                input_dim=in_dim,\n",
        "                num_rbfs=num_rbfs,\n",
        "                output_dim=num_rbfs\n",
        "            ))\n",
        "            layers.append(nn.LayerNorm(num_rbfs))\n",
        "            layers.append(nn.Tanh())\n",
        "            in_dim = num_rbfs\n",
        "\n",
        "        layers.append(nn.Linear(in_dim, out_dim))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        self.skip = nn.Linear(input_dim, out_dim, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass:\n",
        "        u = F_KAN(x) + α * skip(x)\n",
        "        \"\"\"\n",
        "        return self.model(x) + 0.1 * self.skip(x)\n",
        "\n",
        "\n",
        "class VanillaMLP(nn.Module):\n",
        "    \"\"\"Pure data-driven MLP (no physics loss).\"\"\"\n",
        "    def __init__(self, input_dim: int = 2, hidden_dims: List[int] = [64, 64, 32], out_dim: int = 1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "\n",
        "        layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
        "        layers.append(nn.Tanh())\n",
        "\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
        "            layers.append(nn.Tanh())\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dims[-1], out_dim))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class VanillaPINN(nn.Module):\n",
        "    \"\"\"Vanilla PINN (MLP + Physics loss).\"\"\"\n",
        "    def __init__(self, input_dim: int = 2, hidden_dims: List[int] = [64, 64, 32], out_dim: int = 1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "\n",
        "        layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
        "        layers.append(nn.Tanh())\n",
        "\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
        "            layers.append(nn.Tanh())\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dims[-1], out_dim))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "print(\"✅ Model architectures loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX08h0p_Ag-l",
        "outputId": "6aa6b1ea-aade-49b4-a6ec-66278fadd4cf"
      },
      "id": "LX08h0p_Ag-l",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model architectures loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Physics Residuals\n",
        "\n",
        "Here we define the Allen–Cahn PDE. In display form:\n",
        "\n",
        "$$\n",
        "u_t = \\varepsilon^{2}\\, u_{xx} - f(u)\n",
        "$$\n",
        "\n",
        "Equivalently, the physics residual (which the PINN minimizes) can be written as\n",
        "\n",
        "$$\n",
        "\\mathcal{R}(x,t) \\;=\\; u_t(x,t) \\;-\\; \\varepsilon^{2}\\,u_{xx}(x,t) \\;+\\; f\\big(u(x,t)\\big).\n",
        "$$\n",
        "\n",
        "A commonly used choice for the nonlinear reaction term is the double-well potential derivative:\n",
        "\n",
        "$$\n",
        "f(u) = u^{3} - u,\n",
        "$$\n",
        "\n",
        "so that the PDE becomes\n",
        "\n",
        "$$\n",
        "u_t = \\varepsilon^{2}\\,u_{xx} - (u^{3} - u).\n",
        "$$\n",
        "\n",
        "**Purpose:**  \n",
        "The residual enforces physical consistency — the predicted solution must minimize \\(\\mathcal{R}(x,t)\\) across the spatial–temporal domain.\n"
      ],
      "metadata": {
        "id": "rBOwFOEzAFdG"
      },
      "id": "rBOwFOEzAFdG"
    },
    {
      "cell_type": "code",
      "source": [
        "def allen_cahn_pde_residual(model: nn.Module, x: torch.Tensor, t: torch.Tensor, epsilon: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Stable Allen–Cahn residual:\n",
        "        u_t = ε² u_xx - (u³ - u)\n",
        "    Includes:\n",
        "        - shape-safe handling for x,t\n",
        "        - clamped u for stability\n",
        "        - stabilizer λu term\n",
        "        - residual clipping\n",
        "    \"\"\"\n",
        "\n",
        "    if x.dim() == 1:\n",
        "        x = x.unsqueeze(-1)\n",
        "    if t.dim() == 1:\n",
        "        t = t.unsqueeze(-1)\n",
        "\n",
        "    X = torch.cat([x, t], dim=1).clone().detach().requires_grad_(True)\n",
        "\n",
        "    u = model(X)\n",
        "    u = torch.clamp(u, -1.5, 1.5)\n",
        "\n",
        "    grads = torch.autograd.grad(\n",
        "        outputs=u, inputs=X, grad_outputs=torch.ones_like(u),\n",
        "        create_graph=True, retain_graph=True, only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    u_x = grads[:, 0:1]\n",
        "    u_t = grads[:, 1:2]\n",
        "\n",
        "    u_xx = torch.autograd.grad(\n",
        "        outputs=u_x, inputs=X, grad_outputs=torch.ones_like(u_x),\n",
        "        create_graph=True, retain_graph=True, only_inputs=True\n",
        "    )[0][:, 0:1]\n",
        "\n",
        "    stabilizer = 1e-4 * u\n",
        "    residual = u_t - (epsilon**2) * u_xx + u**3 - u + stabilizer\n",
        "    residual = torch.clamp(residual, -5.0, 5.0)\n",
        "\n",
        "    return residual\n",
        "\n",
        "print(\"✅ Improved Physics residual function loaded\")\n",
        "\n",
        "\n",
        "# ==================== LOSS FUNCTIONS ====================\n",
        "\n",
        "def initial_condition_loss(model: nn.Module, x_ic: torch.Tensor, u_ic_true: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    MSE loss enforcing u(x,0) = u0(x)\n",
        "    x_ic: shape (N_ic, 1)\n",
        "    u_ic_true: shape (N_ic, 1)\n",
        "    \"\"\"\n",
        "    t_ic = torch.zeros_like(x_ic).to(x_ic.device)\n",
        "    X_ic = torch.cat([x_ic, t_ic], dim=1)\n",
        "    u_pred = model(X_ic)\n",
        "    return nn.MSELoss()(u_pred, u_ic_true)\n",
        "\n",
        "\n",
        "def boundary_condition_loss(model: nn.Module, x_bc: torch.Tensor, t_bc: torch.Tensor, u_bc_true: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Enforce Dirichlet boundary values: u(x=±L, t) = known (in your data it's 0)\n",
        "    x_bc: (N_bc,1), t_bc: (N_bc,1), u_bc_true: (N_bc,1)\n",
        "    \"\"\"\n",
        "    X_bc = torch.cat([x_bc, t_bc], dim=1).to(x_bc.device)\n",
        "    u_pred = model(X_bc)\n",
        "    return nn.MSELoss()(u_pred, u_bc_true)\n",
        "\n",
        "\n",
        "def data_loss(model: nn.Module, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Pure data loss (for Vanilla MLP).\"\"\"\n",
        "    pred = model(X)\n",
        "    return nn.MSELoss()(pred, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AwB-qVAA4J4",
        "outputId": "2b873244-49af-446b-bfee-931364f89a73"
      },
      "id": "-AwB-qVAA4J4",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Improved Physics residual function loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Loss Function Definitions\n",
        "We now define the composite loss that combines:\n",
        "- **PDE residual loss**\n",
        "- **Initial condition (IC) loss**\n",
        "- **Boundary condition (BC) loss**\n",
        "\n",
        "The total loss guides the model toward both data fidelity and physical correctness.\n"
      ],
      "metadata": {
        "id": "e7FgpQfbAIpW"
      },
      "id": "e7FgpQfbAIpW"
    },
    {
      "cell_type": "code",
      "source": [
        "def pinn_loss(model: nn.Module,\n",
        "              X_data: torch.Tensor, y_data: torch.Tensor,\n",
        "              X_coll: torch.Tensor, epsilon: float,\n",
        "              X_ic: Optional[torch.Tensor] = None,\n",
        "              y_ic: Optional[torch.Tensor] = None,\n",
        "              X_bc: Optional[torch.Tensor] = None,\n",
        "              y_bc: Optional[torch.Tensor] = None,\n",
        "              alpha: float = 0.1,\n",
        "              beta: float = 1.0,\n",
        "              gamma_ic: float = 200.0,\n",
        "              gamma_bc: float = 50.0\n",
        "              ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Improved PINN loss:\n",
        "        - data loss (small)\n",
        "        - physics residual loss (adaptive growing)\n",
        "        - IC loss (strong)\n",
        "        - BC loss (moderate)\n",
        "        - PDE curriculum (adaptive β)\n",
        "    \"\"\"\n",
        "\n",
        "    pred = model(X_data)\n",
        "    loss_data = nn.MSELoss()(pred, y_data)\n",
        "\n",
        "    x_col = X_coll[:, 0]\n",
        "    t_col = X_coll[:, 1]\n",
        "    residual = allen_cahn_pde_residual(model, x_col, t_col, epsilon)\n",
        "    loss_physics = torch.mean(residual**2)\n",
        "\n",
        "    loss_ic = torch.tensor(0.0, device=X_data.device)\n",
        "    if X_ic is not None and y_ic is not None:\n",
        "        loss_ic = nn.MSELoss()(model(X_ic), y_ic)\n",
        "\n",
        "    loss_bc = torch.tensor(0.0, device=X_data.device)\n",
        "    if X_bc is not None and y_bc is not None:\n",
        "        loss_bc = nn.MSELoss()(model(X_bc), y_bc)\n",
        "\n",
        "    global epoch_num\n",
        "    if \"epoch_num\" not in globals():\n",
        "        epoch_num = 0\n",
        "\n",
        "    pde_weight = beta * (1.0 + 0.001 * epoch_num)\n",
        "\n",
        "    total_loss = (\n",
        "        alpha * loss_data +\n",
        "        pde_weight * loss_physics +\n",
        "        gamma_ic * loss_ic +\n",
        "        gamma_bc * loss_bc\n",
        "    )\n",
        "\n",
        "    return total_loss, loss_data, loss_physics, loss_ic, loss_bc\n",
        "\n",
        "print(\"✅ Loss functions loaded\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n28uVEnCBSqk",
        "outputId": "a258fae0-907a-499f-b696-3b0655856b0c"
      },
      "id": "n28uVEnCBSqk",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loss functions loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Training and Results Visualization\n",
        "\n",
        "After defining all components, we train the model using gradient descent.\n",
        "\n",
        "Below, we visualize:\n",
        "- The **predicted vs true** Allen–Cahn field,\n",
        "- The **error map** across the domain,\n",
        "- And optionally, the **training loss curve** over iterations."
      ],
      "metadata": {
        "id": "JqJFJRLFALMu"
      },
      "id": "JqJFJRLFALMu"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vanilla_mlp(model: nn.Module, X_data: torch.Tensor, y_data: torch.Tensor,\n",
        "                      epochs: int = 2000, lr: float = 1e-3,\n",
        "                      print_every: int = 200) -> Dict[str, List[float]]:\n",
        "    \"\"\"Train vanilla MLP (data-driven only).\"\"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = data_loss(model, X_data, y_data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
        "            logger.info(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.6e}\")\n",
        "\n",
        "    return {'loss': loss_history}\n",
        "\n",
        "\n",
        "# ==================== IMPROVED TRAINING LOOP (ADAM + LBFGS) ====================\n",
        "def train_pinn(model: nn.Module, X_data: torch.Tensor, y_data: torch.Tensor,\n",
        "               X_coll: torch.Tensor, epsilon: float,\n",
        "               X_ic: Optional[torch.Tensor] = None, y_ic: Optional[torch.Tensor] = None,\n",
        "               X_bc: Optional[torch.Tensor] = None, y_bc: Optional[torch.Tensor] = None,\n",
        "               epochs: int = 2000, lr: float = 1e-3,\n",
        "               print_every: int = 200) -> Dict[str, List[float]]:\n",
        "    \"\"\"Train PINN with improved Adam + LBFGS optimization.\"\"\"\n",
        "\n",
        "    global epoch_num\n",
        "    epoch_num = 0\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    loss_history = []\n",
        "    data_loss_history = []\n",
        "    physics_loss_history = []\n",
        "    ic_loss_history = []\n",
        "    bc_loss_history = []\n",
        "\n",
        "    print(\"\\n=== Starting Adam optimizer phase ===\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_num = epoch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total, d_loss, p_loss, ic_loss, bc_loss = pinn_loss(\n",
        "            model,\n",
        "            X_data, y_data,\n",
        "            X_coll, epsilon,\n",
        "            X_ic=X_ic, y_ic=y_ic,\n",
        "            X_bc=X_bc, y_bc=y_bc,\n",
        "            alpha=0.1,\n",
        "            beta=1.0,\n",
        "            gamma_ic=200.0,\n",
        "            gamma_bc=50.0\n",
        "        )\n",
        "\n",
        "        total.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_history.append(total.item())\n",
        "        data_loss_history.append(d_loss.item())\n",
        "        physics_loss_history.append(p_loss.item())\n",
        "        ic_loss_history.append(ic_loss.item() if isinstance(ic_loss, torch.Tensor) else ic_loss)\n",
        "        bc_loss_history.append(bc_loss.item() if isinstance(bc_loss, torch.Tensor) else bc_loss)\n",
        "\n",
        "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
        "            print(f\"[Adam] Epoch {epoch+1}/{epochs} | \"\n",
        "                  f\"Total: {total.item():.3e} | \"\n",
        "                  f\"Data: {d_loss.item():.2e} | Physics: {p_loss.item():.2e} | \"\n",
        "                  f\"IC: {ic_loss.item() if isinstance(ic_loss, torch.Tensor) else ic_loss:.2e} | \"\n",
        "                  f\"BC: {bc_loss.item() if isinstance(bc_loss, torch.Tensor) else bc_loss:.2e}\")\n",
        "\n",
        "    print(\"\\n=== Starting LBFGS refinement ===\")\n",
        "\n",
        "    lbfgs = torch.optim.LBFGS(model.parameters(),\n",
        "                              max_iter=500,\n",
        "                              tolerance_grad=1e-9,\n",
        "                              tolerance_change=1e-9,\n",
        "                              history_size=50)\n",
        "\n",
        "    def closure():\n",
        "        lbfgs.zero_grad()\n",
        "        total, *_ = pinn_loss(\n",
        "            model,\n",
        "            X_data, y_data,\n",
        "            X_coll, epsilon,\n",
        "            X_ic=X_ic, y_ic=y_ic,\n",
        "            X_bc=X_bc, y_bc=y_bc,\n",
        "            alpha=0.1,\n",
        "            beta=1.0,\n",
        "            gamma_ic=200.0,\n",
        "            gamma_bc=50.0\n",
        "        )\n",
        "        total.backward()\n",
        "        return total\n",
        "\n",
        "    lbfgs.step(closure)\n",
        "    print(\"LBFGS complete.\\n\")\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss_history,\n",
        "        \"data_loss\": data_loss_history,\n",
        "        \"physics_loss\": physics_loss_history,\n",
        "        \"ic_loss\": ic_loss_history,\n",
        "        \"bc_loss\": bc_loss_history\n",
        "    }\n",
        "\n",
        "print(\"✅ Training functions loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJGWARXXBXai",
        "outputId": "304ccca4-9a4c-4599-b503-b35e41be6dfd"
      },
      "id": "HJGWARXXBXai",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training functions loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(model: nn.Module, X: torch.Tensor, y: torch.Tensor,\n",
        "                   X_coll: torch.Tensor, epsilon: float) -> Dict[str, float]:\n",
        "    \"\"\"Compute comprehensive evaluation metrics.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        u_pred = model(X)\n",
        "\n",
        "        mse = torch.mean((u_pred - y)**2)\n",
        "        rmse = torch.sqrt(mse)\n",
        "        mae = torch.mean(torch.abs(u_pred - y))\n",
        "        rel_l2_error = torch.norm(u_pred - y) / torch.norm(y)\n",
        "\n",
        "    X_coll_eval = X_coll.clone().detach().requires_grad_(True)\n",
        "    x_col = X_coll_eval[:, 0]\n",
        "    t_col = X_coll_eval[:, 1]\n",
        "\n",
        "    residual = allen_cahn_pde_residual(model, x_col, t_col, epsilon)\n",
        "\n",
        "    residual_l2_norm = torch.norm(residual, p=2)\n",
        "    residual_l2_normalized = residual_l2_norm / np.sqrt(len(residual))\n",
        "    max_residual = torch.max(torch.abs(residual))\n",
        "    mean_residual = torch.mean(torch.abs(residual))\n",
        "\n",
        "    metrics = {\n",
        "        'RMSE': rmse.item(),\n",
        "        'MAE': mae.item(),\n",
        "        'Relative_L2_Error': rel_l2_error.item(),\n",
        "        'Residual_L2_Norm': residual_l2_normalized.item(),\n",
        "        'Max_Residual': max_residual.item(),\n",
        "        'Mean_Residual': mean_residual.item()\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "print(\"✅ Evaluation functions loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qoi-cSTBa-z",
        "outputId": "4cf2a14e-3227-4bc0-8a56-96e20a6aaacd"
      },
      "id": "1Qoi-cSTBa-z",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Evaluation functions loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Visualization of Results\n",
        "\n",
        "Here, we visualize:\n",
        "- The predicted field `u_pred(x,t)` over the domain,\n",
        "- The true/reference field `u_true(x,t)`, and\n",
        "- The absolute error distribution `|u_pred - u_true|`.\n",
        "\n",
        "These visualizations help confirm if the learned dynamics replicate the Allen–Cahn diffusion-reaction pattern formation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2AboyhqNAOOd"
      },
      "id": "2AboyhqNAOOd"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_prediction_vs_actual(model: nn.Module, X: torch.Tensor, y: torch.Tensor,\n",
        "                              title: str = \"Predictions vs Actual\"):\n",
        "    \"\"\"Scatter plot of predictions vs actual values.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(X).cpu().numpy()\n",
        "        actual = y.cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(actual, preds, alpha=0.5)\n",
        "    plt.xlabel('Actual u', fontsize=12)\n",
        "    plt.ylabel('Predicted u', fontsize=12)\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{title.replace(' ', '_').replace(':', '')}_scatter.png\")\n",
        "    plt.close() # Close plot to prevent it from displaying in a loop\n",
        "\n",
        "def plot_residual_surface(model, X_coll, epsilon):\n",
        "\n",
        "    x = X_coll[:, 0].detach().cpu().numpy()\n",
        "    t = X_coll[:, 1].detach().cpu().numpy()\n",
        "\n",
        "    res_tensor = allen_cahn_pde_residual(\n",
        "        model,\n",
        "        X_coll[:, 0],\n",
        "        X_coll[:, 1],\n",
        "        epsilon\n",
        "    )\n",
        "    # *** FIX: .flatten() is required for 3D scatter's c and z args ***\n",
        "    res = res_tensor.detach().cpu().numpy().flatten()\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    ax.scatter(x, t, np.abs(res), s=2, c=np.abs(res), cmap='hot')\n",
        "    ax.set_xlabel(\"x\")\n",
        "    ax.set_ylabel(\"t\")\n",
        "    ax.set_zlabel(\"|Residual|\")\n",
        "    ax.set_title(\"PDE Residual Surface\")\n",
        "\n",
        "    plt.savefig(f\"residual_surface_{model.__class__.__name__}.png\")\n",
        "    plt.close() # Close plot\n",
        "\n",
        "\n",
        "def plot_solution_heatmaps(model: nn.Module, X: torch.Tensor, y: torch.Tensor,\n",
        "                          title_prefix: str = \"\"):\n",
        "    \"\"\"Plot predicted, true, and error heatmaps.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        u_pred = model(X).cpu().numpy()\n",
        "\n",
        "    x_vals = X[:, 0].cpu().numpy()\n",
        "    t_vals = X[:, 1].cpu().numpy()\n",
        "    x_unique = np.unique(x_vals)\n",
        "    t_unique = np.unique(t_vals)\n",
        "\n",
        "    X_grid, T_grid = np.meshgrid(x_unique, t_unique)\n",
        "\n",
        "    # Handle non-grid data\n",
        "    if X_grid.size != u_pred.size:\n",
        "        print(f\"Warning: Interpolating for heatmap. Grid size {X_grid.size} != Data size {u_pred.size}\")\n",
        "        U_pred_grid = griddata((x_vals, t_vals), u_pred.flatten(), (X_grid, T_grid), method='cubic')\n",
        "        U_true_grid = griddata((x_vals, t_vals), y.cpu().numpy().flatten(), (X_grid, T_grid), method='cubic')\n",
        "    else:\n",
        "        U_pred_grid = u_pred.reshape(len(t_unique), len(x_unique))\n",
        "        U_true_grid = y.cpu().numpy().reshape(len(t_unique), len(x_unique))\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    im1 = axes[0].contourf(X_grid, T_grid, U_pred_grid, levels=50, cmap='viridis')\n",
        "    axes[0].set_title(f'{title_prefix} Predicted Solution u(x,t)',\n",
        "                      fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('x', fontsize=12)\n",
        "    axes[0].set_ylabel('t', fontsize=12)\n",
        "    plt.colorbar(im1, ax=axes[0], label='u')\n",
        "\n",
        "    im2 = axes[1].contourf(X_grid, T_grid, U_true_grid, levels=50, cmap='viridis')\n",
        "    axes[1].set_title('True Solution u(x,t)', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('x', fontsize=12)\n",
        "    axes[1].set_ylabel('t', fontsize=12)\n",
        "    plt.colorbar(im2, ax=axes[1], label='u')\n",
        "\n",
        "    error = np.abs(U_pred_grid - U_true_grid)\n",
        "    im3 = axes[2].contourf(X_grid, T_grid, error, levels=50, cmap='hot')\n",
        "    axes[2].set_title(f'{title_prefix} Absolute Error',\n",
        "                      fontsize=14, fontweight='bold')\n",
        "    axes[2].set_xlabel('x', fontsize=12)\n",
        "    axes[2].set_ylabel('t', fontsize=12)\n",
        "    plt.colorbar(im3, ax=axes[2], label='Error')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{title_prefix.replace(' ', '_')}_solution_heatmaps.png\")\n",
        "    plt.close() # Close plot\n",
        "\n",
        "\n",
        "def plot_residual_heatmap(model: nn.Module, X_coll: torch.Tensor, epsilon: float,\n",
        "                         title_prefix: str = \"\"):\n",
        "    \"\"\"Plot physics residual heatmap.\"\"\"\n",
        "    X_coll_eval = X_coll.clone().detach().requires_grad_(True)\n",
        "\n",
        "    model.eval()\n",
        "    x_col = X_coll_eval[:, 0]\n",
        "    t_col = X_coll_eval[:, 1]\n",
        "\n",
        "    residual = allen_cahn_pde_residual(model, x_col, t_col, epsilon)\n",
        "\n",
        "    x_coll_np = X_coll[:, 0].cpu().numpy()\n",
        "    t_coll_np = X_coll[:, 1].cpu().numpy()\n",
        "    residual_np = residual.detach().cpu().numpy()\n",
        "\n",
        "    X_grid, T_grid = np.meshgrid(\n",
        "        np.linspace(x_coll_np.min(), x_coll_np.max(), 100),\n",
        "        np.linspace(t_coll_np.min(), t_coll_np.max(), 100)\n",
        "    )\n",
        "    residual_grid = griddata(\n",
        "        (x_coll_np, t_coll_np),\n",
        "        residual_np.flatten(),\n",
        "        (X_grid, T_grid),\n",
        "        method='cubic'\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    im = plt.contourf(X_grid, T_grid, np.abs(residual_grid), levels=50, cmap='hot')\n",
        "    plt.colorbar(im, label='|Residual|')\n",
        "    plt.title(f'{title_prefix} Physics Residual Heatmap',\n",
        "              fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('x', fontsize=12)\n",
        "    plt.ylabel('t', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{title_prefix.replace(' ', '_')}_residual_heatmap.png\")\n",
        "    plt.close() # Close plot\n",
        "\n",
        "    print(f\"Max absolute residual: {np.abs(residual_np).max():.6e}\")\n",
        "    print(f\"Mean absolute residual: {np.abs(residual_np).mean():.6e}\")\n",
        "\n",
        "\n",
        "def plot_loss_curves(loss_dict: Dict[str, List[float]], title: str = \"Training Loss\"):\n",
        "    \"\"\"Plot training loss curves.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    axes[0].plot(loss_dict['loss'], label='Total Loss', linewidth=2)\n",
        "    if 'data_loss' in loss_dict:\n",
        "        axes[0].plot(loss_dict['data_loss'], label='Data Loss', linewidth=2, alpha=0.7)\n",
        "        axes[0].plot(loss_dict['physics_loss'], label='Physics Loss', linewidth=2, alpha=0.7)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title(f'{title} (Log Scale)', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].set_yscale('log')\n",
        "\n",
        "    n = min(500, len(loss_dict['loss']))\n",
        "    axes[1].plot(loss_dict['loss'][-n:], label='Total Loss', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Loss', fontsize=12)\n",
        "    axes[1].set_title(f'{title} (Last {n} Epochs)', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{title.replace(' ', '_').replace(':', '')}_loss_curves.png\")\n",
        "    plt.close() # Close plot\n",
        "\n",
        "\n",
        "def plot_comparison_bar_chart(results: Dict[str, Dict[str, float]]):\n",
        "    \"\"\"Bar chart comparing metrics across models.\"\"\"\n",
        "    models = list(results.keys())\n",
        "    metrics = ['RMSE', 'MAE', 'Relative_L2_Error', 'Max_Residual']\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, metric in enumerate(metrics):\n",
        "        values = [results[model][metric] for model in models]\n",
        "        axes[idx].bar(models, values, color=['blue', 'orange', 'green'][:len(models)])\n",
        "        axes[idx].set_ylabel(metric, fontsize=12)\n",
        "        axes[idx].set_title(f'{metric} Comparison', fontsize=13, fontweight='bold')\n",
        "        axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        for i, v in enumerate(values):\n",
        "            axes[idx].text(i, v, f'{v:.2e}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"metrics_comparison_barchart.png\")\n",
        "    plt.close() # Close plot\n",
        "\n",
        "print(\"✅ Visualization functions loaded\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoNOSpmHBhUb",
        "outputId": "4b2f90f6-db49-4eb9-ee3c-46d174b10804"
      },
      "id": "xoNOSpmHBhUb",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Visualization functions loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Main Pipeline Execution\n",
        "\n",
        "This section orchestrates the entire workflow:\n",
        "1. Initializes the dataset and domain,\n",
        "2. Builds the model architecture,\n",
        "3. Defines loss functions and PDE residuals,\n",
        "4. Trains the model,\n",
        "5. Evaluates and visualizes results.\n",
        "\n",
        "It acts as a single entry point to reproduce all results from scratch, ensuring the process is **modular, repeatable, and scalable**.\n"
      ],
      "metadata": {
        "id": "nmXnwBd5AQwV"
      },
      "id": "nmXnwBd5AQwV"
    },
    {
      "cell_type": "code",
      "source": [
        "def u0(x):\n",
        "    return x**2 * np.cos(np.pi * x)\n",
        "\n",
        "print(\"✅ Initial condition function u0(x) loaded\")\n",
        "\n",
        "\n",
        "# ==================== MAIN PIPELINE (FIXED) ====================\n",
        "\n",
        "def run_allen_cahn_experiment(\n",
        "    data_path: str,\n",
        "    collocation_path: str,\n",
        "    epsilon: float = 0.01,\n",
        "    epochs: int = 2000,\n",
        "    lr: float = 1e-3,\n",
        "    save_dir: str = 'allen_cahn_results'\n",
        ") -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Complete experimental pipeline for Allen-Cahn equation.\n",
        "    *** FIX: This version uses SCALED coordinates for all\n",
        "    PINN-related training, evaluation, and visualization. ***\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"ALLEN-CAHN EQUATION PINN-KAN EXPERIMENT\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\n1. Loading data...\")\n",
        "    try:\n",
        "        collocation_df = pd.read_csv(collocation_path)\n",
        "        full_df = pd.read_csv(data_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: Could not load data file. {e}\")\n",
        "        print(\"Please ensure paths are correct.\")\n",
        "        return {}\n",
        "\n",
        "    X_collocation = torch.tensor(collocation_df[['x', 't']].values, dtype=torch.float32).to(device)\n",
        "    X_full = torch.tensor(full_df[['x', 't']].values, dtype=torch.float32).to(device)\n",
        "    y_full = torch.tensor(full_df['u'].values, dtype=torch.float32).unsqueeze(-1).to(device)\n",
        "\n",
        "    print(f\"   X_full shape: {X_full.shape}\")\n",
        "    print(f\"   y_full shape: {y_full.shape}\")\n",
        "    print(f\"   X_collocation shape: {X_collocation.shape}\")\n",
        "\n",
        "    x_min, x_max = X_full[:, 0].min(), X_full[:, 0].max()\n",
        "    t_min, t_max = X_full[:, 1].min(), X_full[:, 1].max()\n",
        "\n",
        "    def scale_X(X):\n",
        "        Xs = X.clone()\n",
        "        Xs[:, 0] = 2.0 * (X[:, 0] - x_min) / (x_max - x_min) - 1.0\n",
        "        Xs[:, 1] = 2.0 * (X[:, 1] - t_min) / (t_max - t_min) - 1.0\n",
        "        return Xs\n",
        "\n",
        "    x_ic_np = np.unique(full_df['x'].values)\n",
        "    x_ic = torch.tensor(x_ic_np.reshape(-1,1), dtype=torch.float32).to(device)\n",
        "    y_ic_np = u0(x_ic_np).reshape(-1,1).astype(np.float32)\n",
        "    y_ic = torch.tensor(y_ic_np, dtype=torch.float32).to(device)\n",
        "    X_ic = torch.cat([x_ic, torch.zeros_like(x_ic)], dim=1)\n",
        "\n",
        "    t_bc_np = np.unique(full_df['t'].values)\n",
        "    x_left = np.full_like(t_bc_np, fill_value=full_df['x'].min()).reshape(-1,1)\n",
        "    x_right = np.full_like(t_bc_np, fill_value=full_df['x'].max()).reshape(-1,1)\n",
        "    t_bc = t_bc_np.reshape(-1,1).astype(np.float32)\n",
        "\n",
        "    X_bc_left = torch.tensor(np.hstack([x_left, t_bc]), dtype=torch.float32).to(device)\n",
        "    X_bc_right = torch.tensor(np.hstack([x_right, t_bc]), dtype=torch.float32).to(device)\n",
        "    X_bc = torch.cat([X_bc_left, X_bc_right], dim=0)\n",
        "    y_bc = torch.zeros((len(X_bc), 1), dtype=torch.float32, device=device)\n",
        "\n",
        "    X_full_s = scale_X(X_full)\n",
        "    X_coll_s = scale_X(X_collocation)\n",
        "    X_ic_s   = scale_X(X_ic)\n",
        "    X_bc_s   = scale_X(X_bc)\n",
        "\n",
        "    print(\"\\n2. Initializing models...\")\n",
        "    models = {\n",
        "        'PINN-KAN': PINN_KAN(input_dim=2, num_rbfs_list=[30, 40, 30], out_dim=1).to(device),\n",
        "        'Vanilla-MLP': VanillaMLP(input_dim=2, hidden_dims=[64, 64, 32], out_dim=1).to(device),\n",
        "        'Vanilla-PINN': VanillaPINN(input_dim=2, hidden_dims=[64, 64, 32], out_dim=1).to(device)\n",
        "    }\n",
        "\n",
        "    for name, model in models.items():\n",
        "        n_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"   {name}: {n_params:,} parameters\")\n",
        "\n",
        "    print(\"\\n3. Training models...\")\n",
        "    loss_histories = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Training {name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if name == 'Vanilla-MLP':\n",
        "            # *** FIX: Train MLP on UNSCALED data ***\n",
        "            loss_dict = train_vanilla_mlp(model, X_full, y_full, epochs=epochs, lr=lr)\n",
        "        else:\n",
        "            # *** FIX: Train PINNs on SCALED data ***\n",
        "            loss_dict = train_pinn(\n",
        "                model, X_full_s, y_full, X_coll_s, epsilon,\n",
        "                X_ic=X_ic_s, y_ic=y_ic,\n",
        "                X_bc=X_bc_s, y_bc=y_bc,\n",
        "                epochs=epochs, lr=lr,\n",
        "                print_every=200\n",
        "            )\n",
        "\n",
        "        loss_histories[name] = loss_dict\n",
        "\n",
        "    print(\"\\n4. Evaluating models...\")\n",
        "    all_metrics = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nEvaluating {name}...\")\n",
        "\n",
        "        # *** FIX: Evaluate MLP on unscaled, PINNs on scaled ***\n",
        "        if name == 'Vanilla-MLP':\n",
        "            metrics = compute_metrics(model, X_full, y_full, X_collocation, epsilon)\n",
        "        else:\n",
        "            metrics = compute_metrics(model, X_full_s, y_full, X_coll_s, epsilon)\n",
        "\n",
        "        all_metrics[name] = metrics\n",
        "\n",
        "        print(f\"   RMSE: {metrics['RMSE']:.6e}\")\n",
        "        print(f\"   MAE: {metrics['MAE']:.6e}\")\n",
        "        print(f\"   Max Residual: {metrics['Max_Residual']:.6e}\")\n",
        "\n",
        "    print(f\"\\n5. Generating visualizations... (saving to '{save_dir}')\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nVisualizations for {name}:\")\n",
        "\n",
        "        # *** FIX: Plot MLP on unscaled, PINNs on scaled ***\n",
        "        if name == 'Vanilla-MLP':\n",
        "            plot_prediction_vs_actual(model, X_full, y_full, title=f\"{name}: Predictions vs Actual\")\n",
        "            plot_solution_heatmaps(model, X_full, y_full, title_prefix=name)\n",
        "        else:\n",
        "            plot_prediction_vs_actual(model, X_full_s, y_full, title=f\"{name}: Predictions vs Actual\")\n",
        "            plot_solution_heatmaps(model, X_full_s, y_full, title_prefix=name)\n",
        "            plot_residual_heatmap(model, X_coll_s, epsilon, title_prefix=name)\n",
        "\n",
        "        plot_loss_curves(loss_histories[name], title=f\"{name} Training\")\n",
        "\n",
        "    print(\"\\nGenerating comparison chart...\")\n",
        "    plot_comparison_bar_chart(all_metrics)\n",
        "\n",
        "    print(\"\\nPlotting Residual Surfaces...\")\n",
        "    # *** FIX: Pass scaled X_coll_s ***\n",
        "    plot_residual_surface(models[\"PINN-KAN\"], X_coll_s, epsilon)\n",
        "    plot_residual_surface(models[\"Vanilla-PINN\"], X_coll_s, epsilon)\n",
        "\n",
        "    print(f\"\\n6. Saving results to {save_dir}...\")\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model_path = os.path.join(save_dir, f\"{name.lower().replace('-', '_')}_model.pth\")\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    metrics_df = pd.DataFrame(all_metrics).T\n",
        "    metrics_df.to_csv(os.path.join(save_dir, 'metrics_comparison.csv'))\n",
        "\n",
        "    with open(os.path.join(save_dir, 'all_results.pkl'), 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'metrics': all_metrics,\n",
        "            'loss_histories': loss_histories\n",
        "        }, f)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EXPERIMENT COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nFINAL METRICS COMPARISON:\")\n",
        "    print(metrics_df.to_string())\n",
        "\n",
        "    return {\n",
        "        'models': models,\n",
        "        'metrics': all_metrics,\n",
        "        'loss_histories': loss_histories\n",
        "    }\n",
        "\n",
        "print(\"✅ Main pipeline loaded\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOkbgXvoB17i",
        "outputId": "98a7f24d-f11a-4834-ff87-2d4abd2848f0"
      },
      "id": "OOkbgXvoB17i",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Initial condition function u0(x) loaded\n",
            "✅ Main pipeline loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Run Experiment\n",
        "\n",
        "Here, we execute experiments with multiple hyperparameters or configurations (e.g., different network sizes, learning rates, or basis functions).\n",
        "\n",
        "**Goal:**  \n",
        "To identify the most accurate and efficient configuration for solving the Allen–Cahn PDE.\n",
        "\n",
        "Each run logs metrics like:\n",
        "- Training loss\n",
        "- Residual loss\n",
        "- Boundary and initial condition satisfaction\n",
        "- Total runtime\n"
      ],
      "metadata": {
        "id": "zwKZD0uCATyu"
      },
      "id": "zwKZD0uCATyu"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Run complete experiment\n",
        "    # *** NOTE: You must have the data files at these paths ***\n",
        "    results = run_allen_cahn_experiment(\n",
        "        data_path='/content/allen_cahn_1d.csv',\n",
        "        collocation_path='/content/allen_cahn_collocation.csv',\n",
        "        epsilon=0.01,\n",
        "        epochs=2000,\n",
        "        lr=1e-3,\n",
        "        save_dir='Results/allen_cahn_results'\n",
        "    )\n",
        "\n",
        "    print(\"\\n✅ All experiments completed successfully!\")\n",
        "    print(f\"📊 Results saved in 'allen_cahn_results/' directory\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb6tqlYmB5Xh",
        "outputId": "870c8373-5a87-4968-dbb3-1467f0f367e9"
      },
      "id": "eb6tqlYmB5Xh",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ALLEN-CAHN EQUATION PINN-KAN EXPERIMENT\n",
            "======================================================================\n",
            "\n",
            "1. Loading data...\n",
            "   X_full shape: torch.Size([51200, 2])\n",
            "   y_full shape: torch.Size([51200, 1])\n",
            "   X_collocation shape: torch.Size([20000, 2])\n",
            "\n",
            "2. Initializing models...\n",
            "   PINN-KAN: 8,653 parameters\n",
            "   Vanilla-MLP: 6,465 parameters\n",
            "   Vanilla-PINN: 6,465 parameters\n",
            "\n",
            "3. Training models...\n",
            "\n",
            "======================================================================\n",
            "Training PINN-KAN\n",
            "======================================================================\n",
            "\n",
            "=== Starting Adam optimizer phase ===\n",
            "[Adam] Epoch 1/2000 | Total: 9.906e+01 | Data: 5.80e-01 | Physics: 9.24e-02 | IC: 4.49e-01 | BC: 1.82e-01\n",
            "[Adam] Epoch 200/2000 | Total: 2.177e+01 | Data: 1.54e-01 | Physics: 3.28e-02 | IC: 1.05e-01 | BC: 1.57e-02\n",
            "[Adam] Epoch 400/2000 | Total: 2.163e+01 | Data: 1.60e-01 | Physics: 3.22e-02 | IC: 1.05e-01 | BC: 1.18e-02\n",
            "[Adam] Epoch 600/2000 | Total: 2.153e+01 | Data: 1.65e-01 | Physics: 3.43e-02 | IC: 1.05e-01 | BC: 1.06e-02\n",
            "[Adam] Epoch 800/2000 | Total: 2.148e+01 | Data: 1.67e-01 | Physics: 3.85e-02 | IC: 1.04e-01 | BC: 1.09e-02\n",
            "[Adam] Epoch 1000/2000 | Total: 2.144e+01 | Data: 1.73e-01 | Physics: 3.75e-02 | IC: 1.04e-01 | BC: 9.21e-03\n",
            "[Adam] Epoch 1200/2000 | Total: 2.143e+01 | Data: 1.76e-01 | Physics: 3.92e-02 | IC: 1.04e-01 | BC: 9.12e-03\n",
            "[Adam] Epoch 1400/2000 | Total: 2.143e+01 | Data: 1.77e-01 | Physics: 4.27e-02 | IC: 1.04e-01 | BC: 9.84e-03\n",
            "[Adam] Epoch 1600/2000 | Total: 2.143e+01 | Data: 1.81e-01 | Physics: 4.05e-02 | IC: 1.04e-01 | BC: 8.92e-03\n",
            "[Adam] Epoch 1800/2000 | Total: 2.144e+01 | Data: 1.82e-01 | Physics: 4.07e-02 | IC: 1.04e-01 | BC: 8.90e-03\n",
            "[Adam] Epoch 2000/2000 | Total: 2.145e+01 | Data: 1.80e-01 | Physics: 4.35e-02 | IC: 1.04e-01 | BC: 9.65e-03\n",
            "\n",
            "=== Starting LBFGS refinement ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2000 | Loss: 1.737415e-01\n",
            "INFO:pinn_logger:Epoch 1/2000 | Loss: 1.737415e-01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LBFGS complete.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Training Vanilla-MLP\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 200/2000 | Loss: 1.829303e-02\n",
            "INFO:pinn_logger:Epoch 200/2000 | Loss: 1.829303e-02\n",
            "Epoch 400/2000 | Loss: 9.481753e-03\n",
            "INFO:pinn_logger:Epoch 400/2000 | Loss: 9.481753e-03\n",
            "Epoch 600/2000 | Loss: 8.159027e-03\n",
            "INFO:pinn_logger:Epoch 600/2000 | Loss: 8.159027e-03\n",
            "Epoch 800/2000 | Loss: 7.531142e-03\n",
            "INFO:pinn_logger:Epoch 800/2000 | Loss: 7.531142e-03\n",
            "Epoch 1000/2000 | Loss: 7.356492e-03\n",
            "INFO:pinn_logger:Epoch 1000/2000 | Loss: 7.356492e-03\n",
            "Epoch 1200/2000 | Loss: 6.560012e-03\n",
            "INFO:pinn_logger:Epoch 1200/2000 | Loss: 6.560012e-03\n",
            "Epoch 1400/2000 | Loss: 6.258022e-03\n",
            "INFO:pinn_logger:Epoch 1400/2000 | Loss: 6.258022e-03\n",
            "Epoch 1600/2000 | Loss: 5.960371e-03\n",
            "INFO:pinn_logger:Epoch 1600/2000 | Loss: 5.960371e-03\n",
            "Epoch 1800/2000 | Loss: 5.648928e-03\n",
            "INFO:pinn_logger:Epoch 1800/2000 | Loss: 5.648928e-03\n",
            "Epoch 2000/2000 | Loss: 5.277555e-03\n",
            "INFO:pinn_logger:Epoch 2000/2000 | Loss: 5.277555e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Training Vanilla-PINN\n",
            "======================================================================\n",
            "\n",
            "=== Starting Adam optimizer phase ===\n",
            "[Adam] Epoch 1/2000 | Total: 2.745e+01 | Data: 1.45e-01 | Physics: 8.71e-02 | IC: 1.18e-01 | BC: 7.31e-02\n",
            "[Adam] Epoch 200/2000 | Total: 6.020e+00 | Data: 2.37e-01 | Physics: 3.63e-01 | IC: 1.70e-02 | BC: 4.31e-02\n",
            "[Adam] Epoch 400/2000 | Total: 4.551e+00 | Data: 2.07e-01 | Physics: 3.47e-01 | IC: 1.16e-02 | BC: 3.46e-02\n",
            "[Adam] Epoch 600/2000 | Total: 3.203e+00 | Data: 1.91e-01 | Physics: 3.70e-01 | IC: 6.16e-03 | BC: 2.72e-02\n",
            "[Adam] Epoch 800/2000 | Total: 2.001e+00 | Data: 1.84e-01 | Physics: 3.60e-01 | IC: 1.49e-03 | BC: 2.07e-02\n",
            "[Adam] Epoch 1000/2000 | Total: 1.652e+00 | Data: 1.81e-01 | Physics: 3.29e-01 | IC: 4.12e-04 | BC: 1.79e-02\n",
            "[Adam] Epoch 1200/2000 | Total: 1.410e+00 | Data: 1.79e-01 | Physics: 2.83e-01 | IC: 2.17e-04 | BC: 1.45e-02\n",
            "[Adam] Epoch 1400/2000 | Total: 1.249e+00 | Data: 1.73e-01 | Physics: 2.44e-01 | IC: 1.37e-04 | BC: 1.24e-02\n",
            "[Adam] Epoch 1600/2000 | Total: 1.151e+00 | Data: 1.70e-01 | Physics: 2.13e-01 | IC: 1.43e-04 | BC: 1.11e-02\n",
            "[Adam] Epoch 1800/2000 | Total: 1.136e+00 | Data: 1.68e-01 | Physics: 1.94e-01 | IC: 2.51e-04 | BC: 1.05e-02\n",
            "[Adam] Epoch 2000/2000 | Total: 1.053e+00 | Data: 1.71e-01 | Physics: 1.75e-01 | IC: 1.37e-04 | BC: 9.66e-03\n",
            "\n",
            "=== Starting LBFGS refinement ===\n",
            "LBFGS complete.\n",
            "\n",
            "\n",
            "4. Evaluating models...\n",
            "\n",
            "Evaluating PINN-KAN...\n",
            "   RMSE: 4.266662e-01\n",
            "   MAE: 3.010126e-01\n",
            "   Max Residual: 3.223138e-01\n",
            "\n",
            "Evaluating Vanilla-MLP...\n",
            "   RMSE: 7.263360e-02\n",
            "   MAE: 2.383815e-02\n",
            "   Max Residual: 4.799616e-01\n",
            "\n",
            "Evaluating Vanilla-PINN...\n",
            "   RMSE: 4.278724e-01\n",
            "   MAE: 2.917583e-01\n",
            "   Max Residual: 5.000000e+00\n",
            "\n",
            "5. Generating visualizations... (saving to 'Results/allen_cahn_results')\n",
            "\n",
            "Visualizations for PINN-KAN:\n",
            "Max absolute residual: 3.223138e-01\n",
            "Mean absolute residual: 1.886590e-01\n",
            "\n",
            "Visualizations for Vanilla-MLP:\n",
            "\n",
            "Visualizations for Vanilla-PINN:\n",
            "Max absolute residual: 5.000000e+00\n",
            "Mean absolute residual: 5.167631e-02\n",
            "\n",
            "Generating comparison chart...\n",
            "\n",
            "Plotting Residual Surfaces...\n",
            "\n",
            "6. Saving results to Results/allen_cahn_results...\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "FINAL METRICS COMPARISON:\n",
            "                  RMSE       MAE  Relative_L2_Error  Residual_L2_Norm  Max_Residual  Mean_Residual\n",
            "PINN-KAN      0.426666  0.301013           0.969290          0.204259      0.322314       0.188659\n",
            "Vanilla-MLP   0.072634  0.023838           0.165007          0.087334      0.479962       0.052175\n",
            "Vanilla-PINN  0.427872  0.291758           0.972030          0.186318      5.000000       0.051676\n",
            "\n",
            "✅ All experiments completed successfully!\n",
            "📊 Results saved in 'allen_cahn_results/' directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U6FT8ycQAFE3"
      },
      "id": "U6FT8ycQAFE3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}