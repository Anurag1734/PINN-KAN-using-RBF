{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1bdfb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Complete PINN-KAN Pipeline for Burgers Equation\n",
    "Includes: PINN-KAN, Vanilla MLP, Vanilla PINN\n",
    "With comprehensive visualization and metrics\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import os\n",
    "import pickle\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad076902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFEdge(nn.Module):\n",
    "    \"\"\"Gaussian Radial Basis Function layer.\"\"\"\n",
    "    def __init__(self, input_dim: int, num_rbfs: int):\n",
    "        super().__init__()\n",
    "        self.centers = nn.Parameter(torch.randn(num_rbfs, input_dim))\n",
    "        self.sigmas = nn.Parameter(torch.ones(num_rbfs, input_dim))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch, input_dim)\n",
    "        # centers: (num_rbfs, input_dim)\n",
    "        expanded = (x.unsqueeze(1) - self.centers) ** 2  # (batch, num_rbfs, input_dim)\n",
    "        scaled = expanded / (2 * (self.sigmas ** 2))\n",
    "        return torch.exp(-scaled.sum(dim=-1))  # (batch, num_rbfs)\n",
    "\n",
    "\n",
    "class KANLayer(nn.Module):\n",
    "    \"\"\"KAN layer with RBF edge.\"\"\"\n",
    "    def __init__(self, input_dim: int, num_rbfs: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.rbf_edge = RBFEdge(input_dim, num_rbfs)\n",
    "        self.linear = nn.Linear(num_rbfs, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        phi = self.rbf_edge(x)\n",
    "        return self.linear(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c685d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_KAN(nn.Module):\n",
    "    \"\"\"Physics-Informed KAN for Burgers equation.\"\"\"\n",
    "    def __init__(self, input_dim: int = 2, num_rbfs_list: List[int] = [20, 30, 15], \n",
    "                 out_dim: int = 1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # First layer\n",
    "        layers.append(KANLayer(input_dim, num_rbfs_list[0], num_rbfs_list[1]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, len(num_rbfs_list) - 1):\n",
    "            layers.append(KANLayer(num_rbfs_list[i], num_rbfs_list[i], num_rbfs_list[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(num_rbfs_list[-1], out_dim))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class VanillaMLP(nn.Module):\n",
    "    \"\"\"Pure data-driven MLP (no physics loss).\"\"\"\n",
    "    def __init__(self, input_dim: int = 2, hidden_dims: List[int] = [64, 64, 32], \n",
    "                 out_dim: int = 1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        layers.append(nn.Tanh())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], out_dim))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class VanillaPINN(nn.Module):\n",
    "    \"\"\"Vanilla PINN (MLP + Physics loss).\"\"\"\n",
    "    def __init__(self, input_dim: int = 2, hidden_dims: List[int] = [64, 64, 32], \n",
    "                 out_dim: int = 1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        layers.append(nn.Tanh())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], out_dim))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b78a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def burgers_pde_residual(model: nn.Module, x: torch.Tensor, t: torch.Tensor, \n",
    "                         nu: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute Burgers equation residual: u_t + u*u_x - nu*u_xx = 0\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        x: Spatial coordinate\n",
    "        t: Temporal coordinate\n",
    "        nu: Viscosity coefficient\n",
    "    \n",
    "    Returns:\n",
    "        PDE residual tensor\n",
    "    \"\"\"\n",
    "    X = torch.stack([x, t], dim=1).requires_grad_()\n",
    "    u = model(X)\n",
    "    \n",
    "    # First derivatives\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=u, inputs=X, grad_outputs=torch.ones_like(u),\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    u_x = grads[:, 0:1]\n",
    "    u_t = grads[:, 1:2]\n",
    "    \n",
    "    # Second derivative\n",
    "    u_xx = torch.autograd.grad(\n",
    "        outputs=u_x, inputs=X, grad_outputs=torch.ones_like(u_x),\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0][:, 0:1]\n",
    "    \n",
    "    # Burgers equation residual\n",
    "    residual = u_t + u * u_x - nu * u_xx\n",
    "    \n",
    "    return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab9ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LOSS FUNCTIONS ====================\n",
    "\n",
    "def data_loss(model: nn.Module, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Pure data loss (for Vanilla MLP).\"\"\"\n",
    "    pred = model(X)\n",
    "    return nn.MSELoss()(pred, y)\n",
    "\n",
    "\n",
    "def pinn_loss(model: nn.Module, X_data: torch.Tensor, y_data: torch.Tensor,\n",
    "              X_coll: torch.Tensor, nu: float, alpha: float = 1.0, \n",
    "              beta: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Combined PINN loss: data + physics\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network\n",
    "        X_data: Data points (x, t)\n",
    "        y_data: True solution values\n",
    "        X_coll: Collocation points\n",
    "        nu: Viscosity\n",
    "        alpha: Weight for data loss\n",
    "        beta: Weight for physics loss\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (total_loss, data_loss, physics_loss)\n",
    "    \"\"\"\n",
    "    # Data loss\n",
    "    pred = model(X_data)\n",
    "    loss_data = nn.MSELoss()(pred, y_data)\n",
    "    \n",
    "    # Physics loss\n",
    "    x_col = X_coll[:, 0]\n",
    "    t_col = X_coll[:, 1]\n",
    "    physics_residual = burgers_pde_residual(model, x_col, t_col, nu)\n",
    "    loss_physics = torch.mean(physics_residual**2)\n",
    "    \n",
    "    # Total loss\n",
    "    total = alpha * loss_data + beta * loss_physics\n",
    "    \n",
    "    return total, loss_data, loss_physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8886b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TRAINING ====================\n",
    "\n",
    "def train_vanilla_mlp(model: nn.Module, X_data: torch.Tensor, y_data: torch.Tensor,\n",
    "                      epochs: int = 2000, lr: float = 1e-3, \n",
    "                      print_every: int = 200) -> Dict[str, List[float]]:\n",
    "    \"\"\"Train vanilla MLP (data-driven only).\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = data_loss(model, X_data, y_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.6e}\")\n",
    "    \n",
    "    return {'loss': loss_history}\n",
    "\n",
    "\n",
    "def train_pinn(model: nn.Module, X_data: torch.Tensor, y_data: torch.Tensor,\n",
    "               X_coll: torch.Tensor, nu: float, epochs: int = 2000, lr: float = 1e-3,\n",
    "               alpha: float = 1.0, beta: float = 1.0, \n",
    "               print_every: int = 200) -> Dict[str, List[float]]:\n",
    "    \"\"\"Train PINN model (data + physics).\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_history = []\n",
    "    data_loss_history = []\n",
    "    physics_loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss, d_loss, p_loss = pinn_loss(\n",
    "            model, X_data, y_data, X_coll, nu, alpha=alpha, beta=beta\n",
    "        )\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_history.append(total_loss.item())\n",
    "        data_loss_history.append(d_loss.item())\n",
    "        physics_loss_history.append(p_loss.item())\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Total: {total_loss.item():.6e} | \"\n",
    "                  f\"Data: {d_loss.item():.6e} | Physics: {p_loss.item():.6e}\")\n",
    "    \n",
    "    return {\n",
    "        'loss': loss_history,\n",
    "        'data_loss': data_loss_history,\n",
    "        'physics_loss': physics_loss_history\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27720354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== EVALUATION ====================\n",
    "\n",
    "def compute_metrics(model: nn.Module, X: torch.Tensor, y: torch.Tensor,\n",
    "                   X_coll: torch.Tensor, nu: float) -> Dict[str, float]:\n",
    "    \"\"\"Compute comprehensive evaluation metrics.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        u_pred = model(X)\n",
    "        \n",
    "        # Prediction metrics\n",
    "        mse = torch.mean((u_pred - y)**2)\n",
    "        rmse = torch.sqrt(mse)\n",
    "        mae = torch.mean(torch.abs(u_pred - y))\n",
    "        rel_l2_error = torch.norm(u_pred - y) / torch.norm(y)\n",
    "    \n",
    "    # Physics residual metrics (requires gradients)\n",
    "    X_coll_eval = X_coll.clone().detach().requires_grad_(True)\n",
    "    x_col = X_coll_eval[:, 0]\n",
    "    t_col = X_coll_eval[:, 1]\n",
    "    \n",
    "    residual = burgers_pde_residual(model, x_col, t_col, nu)\n",
    "    \n",
    "    residual_l2_norm = torch.norm(residual, p=2)\n",
    "    residual_l2_normalized = residual_l2_norm / np.sqrt(len(residual))\n",
    "    max_residual = torch.max(torch.abs(residual))\n",
    "    mean_residual = torch.mean(torch.abs(residual))\n",
    "    \n",
    "    metrics = {\n",
    "        'RMSE': rmse.item(),\n",
    "        'MAE': mae.item(),\n",
    "        'Relative_L2_Error': rel_l2_error.item(),\n",
    "        'Residual_L2_Norm': residual_l2_normalized.item(),\n",
    "        'Max_Residual': max_residual.item(),\n",
    "        'Mean_Residual': mean_residual.item()\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf13d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATION ====================\n",
    "\n",
    "def plot_prediction_vs_actual(model: nn.Module, X: torch.Tensor, y: torch.Tensor,\n",
    "                              title: str = \"Predictions vs Actual\"):\n",
    "    \"\"\"Scatter plot of predictions vs actual values.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X).cpu().numpy()\n",
    "        actual = y.cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(actual, preds, alpha=0.5)\n",
    "    plt.xlabel('Actual u', fontsize=12)\n",
    "    plt.ylabel('Predicted u', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_solution_heatmaps(model: nn.Module, X: torch.Tensor, y: torch.Tensor,\n",
    "                          title_prefix: str = \"\"):\n",
    "    \"\"\"Plot predicted, true, and error heatmaps.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        u_pred = model(X).cpu().numpy()\n",
    "    \n",
    "    # Extract coordinates\n",
    "    x_vals = X[:, 0].cpu().numpy()\n",
    "    t_vals = X[:, 1].cpu().numpy()\n",
    "    x_unique = np.unique(x_vals)\n",
    "    t_unique = np.unique(t_vals)\n",
    "    \n",
    "    # Create grids\n",
    "    X_grid, T_grid = np.meshgrid(x_unique, t_unique)\n",
    "    U_pred_grid = u_pred.reshape(len(t_unique), len(x_unique))\n",
    "    U_true_grid = y.cpu().numpy().reshape(len(t_unique), len(x_unique))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Predicted solution\n",
    "    im1 = axes[0].contourf(X_grid, T_grid, U_pred_grid, levels=50, cmap='viridis')\n",
    "    axes[0].set_title(f'{title_prefix} Predicted Solution u(x,t)', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('x', fontsize=12)\n",
    "    axes[0].set_ylabel('t', fontsize=12)\n",
    "    plt.colorbar(im1, ax=axes[0], label='u')\n",
    "    \n",
    "    # True solution\n",
    "    im2 = axes[1].contourf(X_grid, T_grid, U_true_grid, levels=50, cmap='viridis')\n",
    "    axes[1].set_title('True Solution u(x,t)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('x', fontsize=12)\n",
    "    axes[1].set_ylabel('t', fontsize=12)\n",
    "    plt.colorbar(im2, ax=axes[1], label='u')\n",
    "    \n",
    "    # Absolute error\n",
    "    error = np.abs(U_pred_grid - U_true_grid)\n",
    "    im3 = axes[2].contourf(X_grid, T_grid, error, levels=50, cmap='hot')\n",
    "    axes[2].set_title(f'{title_prefix} Absolute Error', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('x', fontsize=12)\n",
    "    axes[2].set_ylabel('t', fontsize=12)\n",
    "    plt.colorbar(im3, ax=axes[2], label='Error')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_residual_heatmap(model: nn.Module, X_coll: torch.Tensor, nu: float,\n",
    "                         title_prefix: str = \"\"):\n",
    "    \"\"\"Plot physics residual heatmap.\"\"\"\n",
    "    X_coll_eval = X_coll.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    model.eval()\n",
    "    u_coll = model(X_coll_eval)\n",
    "    \n",
    "    # Compute gradients\n",
    "    u_x = torch.autograd.grad(u_coll.sum(), X_coll_eval, create_graph=True)[0][:, 0:1]\n",
    "    u_t = torch.autograd.grad(u_coll.sum(), X_coll_eval, create_graph=True)[0][:, 1:2]\n",
    "    u_xx = torch.autograd.grad(u_x.sum(), X_coll_eval, create_graph=True)[0][:, 0:1]\n",
    "    \n",
    "    # Residual\n",
    "    residual = u_t + u_coll * u_x - nu * u_xx\n",
    "    \n",
    "    # Extract coordinates\n",
    "    x_coll = X_coll[:, 0].cpu().numpy()\n",
    "    t_coll = X_coll[:, 1].cpu().numpy()\n",
    "    residual_np = residual.detach().cpu().numpy()\n",
    "    \n",
    "    # Interpolate for smooth heatmap\n",
    "    X_grid, T_grid = np.meshgrid(\n",
    "        np.linspace(x_coll.min(), x_coll.max(), 100),\n",
    "        np.linspace(t_coll.min(), t_coll.max(), 100)\n",
    "    )\n",
    "    residual_grid = griddata(\n",
    "        (x_coll, t_coll), \n",
    "        residual_np.flatten(), \n",
    "        (X_grid, T_grid), \n",
    "        method='cubic'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    im = plt.contourf(X_grid, T_grid, np.abs(residual_grid), levels=50, cmap='hot')\n",
    "    plt.colorbar(im, label='|Residual|')\n",
    "    plt.title(f'{title_prefix} Physics Residual Heatmap', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('x', fontsize=12)\n",
    "    plt.ylabel('t', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Max absolute residual: {np.abs(residual_np).max():.6e}\")\n",
    "    print(f\"Mean absolute residual: {np.abs(residual_np).mean():.6e}\")\n",
    "\n",
    "\n",
    "def plot_loss_curves(loss_dict: Dict[str, List[float]], title: str = \"Training Loss\"):\n",
    "    \"\"\"Plot training loss curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Full training history\n",
    "    axes[0].plot(loss_dict['loss'], label='Total Loss', linewidth=2)\n",
    "    if 'data_loss' in loss_dict:\n",
    "        axes[0].plot(loss_dict['data_loss'], label='Data Loss', \n",
    "                    linewidth=2, alpha=0.7)\n",
    "        axes[0].plot(loss_dict['physics_loss'], label='Physics Loss', \n",
    "                    linewidth=2, alpha=0.7)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title(f'{title} (Log Scale)', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_yscale('log')\n",
    "    \n",
    "    # Last 500 epochs\n",
    "    n = min(500, len(loss_dict['loss']))\n",
    "    axes[1].plot(loss_dict['loss'][-n:], label='Total Loss', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title(f'{title} (Last {n} Epochs)', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_comparison_bar_chart(results: Dict[str, Dict[str, float]]):\n",
    "    \"\"\"Bar chart comparing metrics across models.\"\"\"\n",
    "    models = list(results.keys())\n",
    "    metrics = ['RMSE', 'MAE', 'Relative_L2_Error', 'Max_Residual']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        values = [results[model][metric] for model in models]\n",
    "        axes[idx].bar(models, values, color=['blue', 'orange', 'green'][:len(models)])\n",
    "        axes[idx].set_ylabel(metric, fontsize=12)\n",
    "        axes[idx].set_title(f'{metric} Comparison', fontsize=13, fontweight='bold')\n",
    "        axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(values):\n",
    "            axes[idx].text(i, v, f'{v:.2e}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003459a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MAIN PIPELINE ====================\n",
    "\n",
    "def run_burgers_experiment(\n",
    "    data_path: str,\n",
    "    collocation_path: str,\n",
    "    nu: float = 0.01/np.pi,\n",
    "    epochs: int = 2000,\n",
    "    lr: float = 1e-3,\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 1.0,\n",
    "    save_dir: str = 'burgers_results'\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Complete experimental pipeline for Burgers equation.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with models, metrics, and loss histories\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"BURGERS EQUATION PINN-KAN EXPERIMENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    collocation_df = pd.read_csv(collocation_path)\n",
    "    X_collocation = torch.tensor(collocation_df.values, dtype=torch.float32).to(device)\n",
    "    \n",
    "    full_df = pd.read_csv(data_path)\n",
    "    X_full = torch.tensor(full_df[['x', 't']].values, dtype=torch.float32).to(device)\n",
    "    y_full = torch.tensor(full_df['u'].values, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "    \n",
    "    print(f\"   X_full shape: {X_full.shape}\")\n",
    "    print(f\"   y_full shape: {y_full.shape}\")\n",
    "    print(f\"   X_collocation shape: {X_collocation.shape}\")\n",
    "    \n",
    "    # Initialize models\n",
    "    print(\"\\n2. Initializing models...\")\n",
    "    models = {\n",
    "        'PINN-KAN': PINN_KAN(input_dim=2, num_rbfs_list=[20, 30, 15], out_dim=1).to(device),\n",
    "        'Vanilla-MLP': VanillaMLP(input_dim=2, hidden_dims=[64, 64, 32], out_dim=1).to(device),\n",
    "        'Vanilla-PINN': VanillaPINN(input_dim=2, hidden_dims=[64, 64, 32], out_dim=1).to(device)\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        n_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"   {name}: {n_params:,} parameters\")\n",
    "    \n",
    "    # Train models\n",
    "    print(\"\\n3. Training models...\")\n",
    "    loss_histories = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Training {name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        if name == 'Vanilla-MLP':\n",
    "            loss_dict = train_vanilla_mlp(model, X_full, y_full, epochs=epochs, lr=lr)\n",
    "        else:\n",
    "            loss_dict = train_pinn(model, X_full, y_full, X_collocation, nu, \n",
    "                                  epochs=epochs, lr=lr, alpha=alpha, beta=beta)\n",
    "        \n",
    "        loss_histories[name] = loss_dict\n",
    "    \n",
    "    # Evaluate models\n",
    "    print(\"\\n4. Evaluating models...\")\n",
    "    all_metrics = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "        metrics = compute_metrics(model, X_full, y_full, X_collocation, nu)\n",
    "        all_metrics[name] = metrics\n",
    "        \n",
    "        print(f\"   RMSE: {metrics['RMSE']:.6e}\")\n",
    "        print(f\"   MAE: {metrics['MAE']:.6e}\")\n",
    "        print(f\"   Max Residual: {metrics['Max_Residual']:.6e}\")\n",
    "    \n",
    "    # Visualizations\n",
    "    print(\"\\n5. Generating visualizations...\")\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nVisualizations for {name}:\")\n",
    "        \n",
    "        # Prediction vs actual\n",
    "        plot_prediction_vs_actual(model, X_full, y_full, title=f\"{name}: Predictions vs Actual\")\n",
    "        \n",
    "        # Solution heatmaps\n",
    "        plot_solution_heatmaps(model, X_full, y_full, title_prefix=name)\n",
    "        \n",
    "        # Residual heatmap (only for PINN models)\n",
    "        if 'PINN' in name or 'KAN' in name:\n",
    "            plot_residual_heatmap(model, X_collocation, nu, title_prefix=name)\n",
    "        \n",
    "        # Loss curves\n",
    "        plot_loss_curves(loss_histories[name], title=f\"{name} Training\")\n",
    "    \n",
    "    # Comparison plot\n",
    "    print(\"\\nGenerating comparison chart...\")\n",
    "    plot_comparison_bar_chart(all_metrics)\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"\\n6. Saving results to {save_dir}...\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save models\n",
    "    for name, model in models.items():\n",
    "        model_path = os.path.join(save_dir, f\"{name.lower().replace('-', '_')}_model.pth\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_df = pd.DataFrame(all_metrics).T\n",
    "    metrics_df.to_csv(os.path.join(save_dir, 'metrics_comparison.csv'))\n",
    "    \n",
    "    with open(os.path.join(save_dir, 'all_results.pkl'), 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'metrics': all_metrics,\n",
    "            'loss_histories': loss_histories\n",
    "        }, f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT COMPLETED!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\nFINAL METRICS COMPARISON:\")\n",
    "    print(metrics_df.to_string())\n",
    "    \n",
    "    return {\n",
    "        'models': models,\n",
    "        'metrics': all_metrics,\n",
    "        'loss_histories': loss_histories\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e1d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BURGERS EQUATION PINN-KAN EXPERIMENT\n",
      "======================================================================\n",
      "\n",
      "1. Loading data...\n",
      "   X_full shape: torch.Size([25600, 2])\n",
      "   y_full shape: torch.Size([25600, 1])\n",
      "   X_collocation shape: torch.Size([20000, 2])\n",
      "\n",
      "2. Initializing models...\n",
      "   PINN-KAN: 2,991 parameters\n",
      "   Vanilla-MLP: 6,465 parameters\n",
      "   Vanilla-PINN: 6,465 parameters\n",
      "\n",
      "3. Training models...\n",
      "\n",
      "======================================================================\n",
      "Training PINN-KAN\n",
      "======================================================================\n",
      "Epoch 1/2000 | Total: 4.973315e-01 | Data: 4.973315e-01 | Physics: 1.800404e-10\n"
     ]
    }
   ],
   "source": [
    "# ==================== USAGE EXAMPLE ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run complete experiment\n",
    "    results = run_burgers_experiment(\n",
    "        data_path='../data/burgers_full.csv',\n",
    "        collocation_path='../data/burgers_collocation.csv',\n",
    "        nu=0.01/np.pi,\n",
    "        epochs=2000,\n",
    "        lr=1e-3,\n",
    "        alpha=1.0,\n",
    "        beta=1.0,\n",
    "        save_dir='../Results/burgers_results'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… All experiments completed successfully!\")\n",
    "    print(f\"ðŸ“Š Results saved in 'burgers_results/' directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a6d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492256b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed717f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
